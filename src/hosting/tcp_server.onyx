package http.server

use runtime

TCP_Server :: struct {
    server: &net.TCP_Server;
    pipeline: Pipeline;

    thread_pool: [] thread.Thread;

    ready_clients:   [..] &net.TCP_Server.Client;
    ready_mutex:     sync.Mutex;
    ready_condvar:   sync.Condition_Variable;

    max_clients: i32;
}

TCP_Server_Settings :: struct {
    max_clients := 32;
    thread_count := 0;
}

tcp :: #match {}

#overload
tcp :: (pipeline: Pipeline, settings := TCP_Server_Settings.{}) -> &TCP_Server {
    threads := make([] thread.Thread, settings.thread_count);

    server := long_term_allocator->move(TCP_Server.{
        pipeline = pipeline,
        thread_pool = threads,
        max_clients = settings.max_clients
    });

    sync.condition_init(&server.ready_condvar);
    sync.mutex_init(&server.ready_mutex);
    array.init(&server.ready_clients, 4, long_term_allocator);

    for& t in threads {
        thread.spawn(t, server, tcp_server_thread);
    }

    return server;
}

#overload
tcp :: (p: $P/Pipeable, settings := TCP_Server_Settings.{}) -> &TCP_Server {
    pipe := pipeline();
    pipe->pipe(p);
    return tcp(pipe, settings);
}




Serve_Status :: enum {
    Success;
    Failed_To_Bind;
}

#inject
TCP_Server.serve :: (tcp: &TCP_Server, port: u32) -> Serve_Status {
    tcp.server = net.tcp_server_make(max_clients=tcp.max_clients);
    tcp.server.emit_data_events = false;

    // Allows the server to be immediately restarted. Not
    // the safest, but it is fine for a development instance.
    tcp.server.socket->option(.ReuseAddress, true);

    if !tcp.server->listen(~~port) {
        log(.Error, "HTTP-Server", core.tprintf("Failed to bind to port {}.\n", port));
        return .Failed_To_Bind;
    }

    log(.Info, "HTTP-Server", core.tprintf("Serving on port {}", port));
    tcp.server->handle_events() {
        case .Ready {
            ready_client := cast(&core.net.TCP_Event.Ready, it.data).client;
            if ready_client.socket.vtable == null || ready_client.state != .Alive {
                log(.Warning, "HTTP-Server", "Discarding dead client");
                ready_client->read_complete();
                continue;
            }

            if !tcp.thread_pool {
                handle_request(tcp, ready_client);

            } else {
                sync.critical_section(&tcp.ready_mutex) {
                    array.push(&tcp.ready_clients, ready_client);
                    sync.condition_broadcast(&tcp.ready_condvar);
                }
            }
        }
    }

    return .Success;
}

#local
tcp_server_thread :: (tcp: &TCP_Server) {
    client: #auto;
    while true {
        sync.critical_section(&tcp.ready_mutex) {
            while (tcp.ready_clients.length == 0) {
                sync.condition_wait(&tcp.ready_condvar, &tcp.ready_mutex);
            }

            client = tcp.ready_clients[0];
            array.delete(&tcp.ready_clients, 0);
        }

        handle_request(tcp, client);
    }
}

#local
handle_request :: (tcp: &TCP_Server, client: &net.TCP_Server.Client) {
    if !inner_handle_request(tcp, client) {
        if !client.socket.alive || client.socket.vtable == null {
            return;
        }

        // Close the connection if we failed to read the request.
        io.stream_write(&client.socket, "HTTP/1.1 500 Internal Server Error\r\nConnection: close\r\n\r\n");
        tcp.server->kill_client(client);

    } else {
        client->read_complete();
    }
}

#local
inner_handle_request :: (tcp: &TCP_Server, client: &net.TCP_Server.Client) -> bool {
    // If the stream is no longer set up correctly, abandon it.
    if !client.socket.alive || client.socket.vtable == null {
        return false;
    }

    session_gc := alloc.gc.make();

    use_gc_allocator_in_current_scope(&session_gc);
    defer alloc.clear_temp_allocator();

    // We have to copy the client's socket because the request handler
    // has the ability to call TCP_Server.Client.detach which frees the
    // client and releases control of the socket to the caller. This
    // enables WebSockets and similar protocols. However, here we still
    // need to respond to the request, so we must copy the socket to
    // use it later.
    client_addr := client.address;
    client_sock := client.socket;

    reader := io.reader_make(&client_sock);
    defer io.reader_free(&reader);

    //
    // Loop until there is no data buffered in the reader.
    // If two requests were sent back-to-back, there is a non-zero
    // chance the reader will start consuming the second request
    // when reading the body/headers of the first. If it does,
    // immediately process that data here, otherwise it will be
    // lost forever. If the reader doesn't start reading those bytes,
    // then the TCP server will just say that the client is ready
    // to be read from again.
    while #bottom_test io.reader_get_buffered(&reader) > 0 {
        req := Request.{
            client  = client,
            address = .{ client_addr->addr_as_str() }
        };
        defer request_free(&req);

        if !parse_request(&req, &reader) {
            return false;
        }

        res: Response;
        response_init(&res);
        defer response_free(&res);

        // Process the request!
        tcp.pipeline->process(&req, &res);

        // If the body was not read for some reason,
        // we need to read it before we can read the next request.
        if !req.body.read {
            req.body->raw();
        }

        response_finalize(&res);

        send_response(&res, &client_sock);

        // When switching protocols, just bail here.
        // In theory the reader should be empty anyway,
        // but just break to be safe.
        if res.status_code == 101 {
            break;
        }
    }

    return true;
}



#local
parse_request :: #match #local {}

#overload
parse_request :: (req: &Request, s: str) -> bool {
    stream := io.buffer_stream_make(s, fixed=true, write_enabled=false);
    r := io.reader_make(&stream);
    defer io.reader_free(&r);
    
    return parse_request(req, &r);
}

#overload
parse_request :: (req: &Request, r: &io.Reader) -> bool {
    // @ErrorHandling
    {
        request_line := r->read_line();
        if request_line.length == 0 do return false;

        method := string.read_until(&request_line, ' ');
        string.advance(&request_line);

        m := core.array.first(runtime.info.enum_values(HTTP_Method), [v](v.name == method));
        if m == null do req.method = .UNKNOWN;
        else         do req.method = ~~m.value;

        route := string.read_until(&request_line, ' ');
        string.advance(&request_line);

        if string.starts_with(route, "http://") || string.starts_with(route, "https://") {
            string.read_until(&route, "/", 2);
        }

        req.endpoint = string.read_until(&route, '?');
        req._processed_endpoint = process_url_to_route_elems(req.endpoint);
        string.advance(&route);

        parse_url_encoded_key_value(&req.query, &route);
    }

    while true {
        header, value := parse_header(r);
        if header == "" do break;

        if header == "cookie" {
            // Parse cookies
            parse_cookies(value, &req.cookies);
            continue;
        }

        req.headers[header] = value;
    }

    req.body.request = req;
    req.body.reader = r;

    return true;
}



#local
use_gc_allocator_in_current_scope :: macro (gc: &alloc.gc.GCState) {
    old_allocator := context.allocator;
    context.allocator = alloc.as_allocator(gc);

    defer {
        alloc.gc.clear(gc);
        context.allocator = old_allocator;
    }
}

